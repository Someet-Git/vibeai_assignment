{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåø Empathetic Friend - Colab API Server\n",
        "\n",
        "Run your fine-tuned empathetic chatbot as an API with a **public URL**!\n",
        "\n",
        "**Features:**\n",
        "- üöÄ Free T4 GPU from Colab\n",
        "- üåê Public URL via ngrok (share with anyone!)\n",
        "- üí¨ Both API and Chat Interface\n",
        "- ‚ö° Fast inference\n",
        "\n",
        "---\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Run cells 1-4** to install deps and load model\n",
        "2. **Choose an option:**\n",
        "   - **Option A (Gradio)**: Beautiful chat interface with public URL\n",
        "   - **Option B (FastAPI)**: REST API for integration with apps\n",
        "   - **Option C (Both)**: Chat + API together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================\n",
        "%pip install -q transformers accelerate torch gradio fastapi uvicorn pyngrok nest-asyncio\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 2: Configuration - UPDATE THIS!\n",
        "# ============================================================\n",
        "\n",
        "MODEL_ID = \"Someet24/empathetic-qwen3-8b-11-01\"  # Your HuggingFace model\n",
        "\n",
        "# Optional: Set your ngrok auth token for persistent URLs\n",
        "# Get free token at: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = None  # e.g., \"2abc123def456...\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a warm, supportive, and empathetic friend. You listen carefully to what people share and respond with genuine care and understanding.\n",
        "\n",
        "When someone shares their feelings:\n",
        "1. Acknowledge and validate their emotions first\n",
        "2. Show that you understand their situation\n",
        "3. Ask thoughtful follow-up questions\n",
        "4. Offer support without being preachy or giving unsolicited advice\n",
        "5. Never minimize their feelings with phrases like \"just\" or \"at least\"\n",
        "\n",
        "You're not a therapist - you're a caring friend who's always there to listen.\"\"\"\n",
        "\n",
        "print(f\"üì¶ Model: {MODEL_ID}\")\n",
        "print(f\"üîë ngrok token: {'Set' if NGROK_AUTH_TOKEN else 'Not set (will use random URL)'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3: Load the Model\n",
        "# ============================================================\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"üîÑ Loading model: {MODEL_ID}\")\n",
        "print(f\"üñ•Ô∏è  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4: Define Generation Function\n",
        "# ============================================================\n",
        "\n",
        "def generate_response(user_message, history=None, temperature=0.7, max_tokens=256):\n",
        "    \"\"\"Generate an empathetic response.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "    \n",
        "    # Build conversation\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    \n",
        "    for user_msg, assistant_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        if assistant_msg:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "    \n",
        "    # Apply chat template\n",
        "    try:\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    except:\n",
        "        text = f\"System: {SYSTEM_PROMPT}\\n\\n\"\n",
        "        for user_msg, assistant_msg in history:\n",
        "            text += f\"User: {user_msg}\\n\\nAssistant: {assistant_msg}\\n\\n\"\n",
        "        text += f\"User: {user_message}\\n\\nAssistant:\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "    \n",
        "    # Handle Qwen3 thinking tokens\n",
        "    if \"<think>\" in response:\n",
        "        response = response.split(\"</think>\")[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test it!\n",
        "print(\"üß™ Testing generation...\")\n",
        "test_response = generate_response(\"I'm feeling a bit anxious today.\")\n",
        "print(f\"\\nü§ñ Response: {test_response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Option A: Gradio Chat Interface (Easiest!)\n",
        "\n",
        "Run this cell to get a beautiful chat interface with a **public URL** you can share!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION A: Gradio Chat Interface\n",
        "# ============================================================\n",
        "import gradio as gr\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"Gradio chat function.\"\"\"\n",
        "    response = generate_response(message, history)\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"üåø Empathetic Friend\",\n",
        "    description=\"A safe space to share what's on your mind. I'm here to listen without judgment.\",\n",
        "    examples=[\n",
        "        \"I'm feeling overwhelmed with work lately.\",\n",
        "        \"My friend hasn't talked to me in weeks.\",\n",
        "        \"I just got some exciting news!\",\n",
        "        \"I've been feeling lonely since moving to a new city.\",\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        ")\n",
        "\n",
        "# Launch with public URL\n",
        "print(\"üöÄ Starting Gradio server...\")\n",
        "print(\"üìã You'll get a PUBLIC URL below that you can share with anyone!\")\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîå Option B: FastAPI REST API\n",
        "\n",
        "Run these cells to get a REST API you can call from any application.\n",
        "\n",
        "‚ö†Ô∏è **Don't run both Option A and B at the same time!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION B: FastAPI Setup\n",
        "# ============================================================\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Allow nested event loops (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"Empathetic Chat API\", version=\"1.0.0\")\n",
        "\n",
        "# Request/Response models\n",
        "class Message(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    messages: List[Message]\n",
        "    temperature: Optional[float] = 0.7\n",
        "    max_tokens: Optional[int] = 256\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "\n",
        "class SimpleRequest(BaseModel):\n",
        "    message: str\n",
        "    temperature: Optional[float] = 0.7\n",
        "    max_tokens: Optional[int] = 256\n",
        "\n",
        "# Endpoints\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"status\": \"running\", \"model\": MODEL_ID}\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    \"\"\"Chat with conversation history.\"\"\"\n",
        "    history = []\n",
        "    user_message = \"\"\n",
        "    \n",
        "    for msg in request.messages:\n",
        "        if msg.role == \"user\":\n",
        "            if user_message:\n",
        "                history.append((user_message, \"\"))\n",
        "            user_message = msg.content\n",
        "        elif msg.role == \"assistant\":\n",
        "            if user_message:\n",
        "                history.append((user_message, msg.content))\n",
        "                user_message = \"\"\n",
        "    \n",
        "    response = generate_response(user_message, history, request.temperature, request.max_tokens)\n",
        "    return ChatResponse(response=response)\n",
        "\n",
        "@app.post(\"/simple\", response_model=ChatResponse)\n",
        "async def simple_chat(request: SimpleRequest):\n",
        "    \"\"\"Simple single-turn chat.\"\"\"\n",
        "    response = generate_response(request.message, temperature=request.temperature, max_tokens=request.max_tokens)\n",
        "    return ChatResponse(response=response)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\", \"gpu\": torch.cuda.is_available()}\n",
        "\n",
        "print(\"‚úÖ FastAPI app created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Start API Server with Public URL\n",
        "# ============================================================\n",
        "PORT = 8000\n",
        "\n",
        "# Set ngrok auth token if provided\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(PORT)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üåê PUBLIC API URL: {public_url}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nüìã API Endpoints:\")\n",
        "print(f\"   GET  {public_url}/          - Status\")\n",
        "print(f\"   POST {public_url}/simple    - Simple chat\")\n",
        "print(f\"   POST {public_url}/chat      - Chat with history\")\n",
        "print(f\"   GET  {public_url}/health    - Health check\")\n",
        "print(f\"   GET  {public_url}/docs      - Interactive docs\")\n",
        "print(f\"\\nüí° Example curl command:\")\n",
        "print(f'   curl -X POST \"{public_url}/simple\" \\\\')\n",
        "print(f'        -H \"Content-Type: application/json\" \\\\')\n",
        "print(f'        -d \\'{{\"message\": \"I am feeling anxious today\"}}\\'')\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running! The server stops when you stop the cell.\")\n",
        "\n",
        "# Run server (this blocks - server runs until you stop the cell)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìö How to Call the API\n",
        "\n",
        "Replace `YOUR_URL` with your ngrok URL from above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Python Example - Run this from any Python script!\n",
        "python_example = '''\n",
        "import requests\n",
        "\n",
        "API_URL = \"YOUR_NGROK_URL\"  # Replace with your URL\n",
        "\n",
        "# Simple chat\n",
        "response = requests.post(\n",
        "    f\"{API_URL}/simple\",\n",
        "    json={\"message\": \"I'm feeling anxious about my exam tomorrow\"}\n",
        ")\n",
        "print(response.json()[\"response\"])\n",
        "\n",
        "# Chat with history\n",
        "response = requests.post(\n",
        "    f\"{API_URL}/chat\",\n",
        "    json={\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"I had a bad day\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I'm sorry to hear that...\"},\n",
        "            {\"role\": \"user\", \"content\": \"My boss yelled at me\"}\n",
        "        ],\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 256\n",
        "    }\n",
        ")\n",
        "print(response.json()[\"response\"])\n",
        "'''\n",
        "\n",
        "print(\"üêç Python Example:\")\n",
        "print(python_example)\n",
        "\n",
        "# JavaScript Example\n",
        "js_example = '''\n",
        "// JavaScript/Fetch Example\n",
        "const API_URL = \"YOUR_NGROK_URL\";\n",
        "\n",
        "async function chat(message) {\n",
        "    const response = await fetch(`${API_URL}/simple`, {\n",
        "        method: \"POST\",\n",
        "        headers: { \"Content-Type\": \"application/json\" },\n",
        "        body: JSON.stringify({ message })\n",
        "    });\n",
        "    const data = await response.json();\n",
        "    return data.response;\n",
        "}\n",
        "\n",
        "// Usage\n",
        "const reply = await chat(\"I need someone to talk to\");\n",
        "console.log(reply);\n",
        "'''\n",
        "\n",
        "print(\"\\nüìú JavaScript Example:\")\n",
        "print(js_example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
